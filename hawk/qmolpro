#!/bin/sh
prefix='q'
this=`basename $0`
prog=${this#${prefix}}
progpath=${prog}

# configuration
#Include this if workers have different core count to head, or if you want for other reasons to configure manually
#max_tasks_per_node=16 

if [ -z "$max_tasks_per_node" ]; then uname=$(uname); test $uname = Darwin && max_tasks_per_node=$(sysctl -n hw.ncpu) || max_tasks_per_node=$(nproc||echo 1) ; fi

#default
if [ x${PROJECT} = x ]; then
    defaultoptions="-d /scratch/$USER -P none"
else
    defaultoptions="-d /scratch/$USER"
fi
                                                                                
# parse options
systemconfigfile=$0.rc
#echo systemconfigfile=$systemconfigfile
#ls -l $systemconfigfile
configfile=${HOME}/.${this}rc
test -r $systemconfigfile && systemconfigfileoptions=$(cat ${systemconfigfile})
test -r $configfile && configfileoptions=$(cat ${configfile})
#echo systemconfigfileoptions=$systemconfigfileoptions
#echo configfileoptions=$configfileoptions
#echo defaultoptions=$defaultoptions
set -- `getopt gd:vW:t:n:l:q:x:p:P:m:N:h: $defaultoptions $systemconfigfileoptions $configfileoptions $*`
parse_opts=0
MEMORY=
TASKS_PER_NODE=
while [ "$1" != -- ]; do
# echo option $1 $2
  case "$1" in
    -d) eval scratchdir="$2"; shift;;
    -g) debug=1 ;;
    -v) verbose=1 ;;
    -k) keeptmp=1 ;;
    -W) OPTION="$OPTION $2"; shift;;
    -l) SBATCHARGS=$(echo $SBATCHARGS $2 | sed -e '/s/,/ /g'); shift;;
    -m) MEMORY=$2; shift;;
    -n) NOPTION=$2; shift;;
    -N) TASKS_PER_NODE=$2; shift;;
    -t) OPTION="$OPTION $1 $2"; THREADS=$2; shift;;
    -p) progpath="$2"; echo progpath=$progpath; shift;;
    -q) QUEUE=$2; shift;;
    -P) PROJECT=$2; shift;;
    -x) garbage=$2; shift;;
    -h) SBATCHARGS="$SBATCHARGS --time=$2"; shift;;
     *) parse_opts=1;;
  esac
  shift
done
shift

memory_reserve=30
# hardware
if [ x${QUEUE} = xcompute  -o x${QUEUE} = xdev ]; then
    hardware_memory=192
    hardware_cores=40
    SBATCHARGS="$SBATCHARGS --exclusive"
elif [ x${QUEUE} = xhighmem ]; then
    hardware_memory=384
    hardware_cores=40
    SBATCHARGS="$SBATCHARGS --exclusive"
else # should be htc
    QUEUE=htc
    hardware_memory=192
    hardware_cores=40
    if [ x${NOPTION} = x ]; then NOPTION=1 ; fi
    if [ x${MEMORY} = x ]; then
	MEMORY=$(echo "import math; print(int(math.floor(128.0*(${hardware_memory}-${memory_reserve})/${hardware_cores})))"|python2)"m"
    fi
fi

if [ x${NOPTION} = x ]; then NOPTION=$hardware_cores ; fi
if [ x${TASKS_PER_NODE} = x ]; then
    if [ x${MEMORY} = x ]; then
	TASKS_PER_NODE=$hardware_cores
    else
	mem=$(echo ${MEMORY}|sed -e 's/m/*1024*1024/' -e 's/g/*1024*1024*1024/' -e 's/$/*8))/' -e 's/^/import math; print int(math.ceil(1.0*/' |python2)
	TASKS_PER_NODE=$(echo "import math; print(int(math.floor((${hardware_memory}-${memory_reserve})*1024.*1024.*1024./${mem})))"|python2)
    fi
fi

if [ ${TASKS_PER_NODE} -gt ${NOPTION} ]; then TASKS_PER_NODE=${NOPTION}; fi
if [ ${NOPTION} -gt ${TASKS_PER_NODE} ]; then 
SBATCHARGS="$SBATCHARGS --exclusive";
if [ x${QUEUE} = xhtc ]; then QUEUE=compute ; fi
fi

if [ x${MEMORY} = x ]; then
    MEMORY=$(echo "import math; print(int(math.floor(128.0*(${hardware_memory}-${memory_reserve})/${TASKS_PER_NODE})))"|python2)"m"
fi

if [ x$verbose != x ]; then
    echo QUEUE=$QUEUE
    echo MEMORY=$MEMORY
    echo TASKS_PER_NODE=$TASKS_PER_NODE
    echo NOPTION=$NOPTION
fi
					       

if [ x${PROJECT} == xnone ]; then # use the special value 'none' in qmolpro.rc to denote that -P must be given later
  echo "Project missing. Specify it with -P (possible in $HOME/.${this}rc), or via the PROJECT environment variable"
  exit 1;
fi

if [ x${garbage} != x -a x${scratchdir} != x ]; then # garbage-collect any outstanding scratch directories
#fixme
    find $scratchdir -name 'SLURM.*' -mtime -"$garbage" -print0 | xargs -0  rm -rf
fi

if [ $parse_opts -eq 1 -o x"$*" = x ] ; then
cat << EOF
Usage: $0 -n tasks -h HH[:MM[:SS]] [-v] [-m memory] [-q partition] [-P project] [-l further,sbatch,options] [-W MOLPRO_OPTION] INPUT_FILES

Submits one or more Molpro input files into the batch system. If no arguments
are specified, a serial job is sent to the htc Slurm partition. The job
will use the molpro found first in \$PATH of the submitting shell.

PRINCIPAL OPTIONS

  -n Number of MPI processes desired. Default is 1 if job goes to htc partition,
     otherwise the number of processes that can fit in one dedicated node,
     taking into account the -m option.

  -m The memory allocated to each MPI process, in units of 8-byte words. The
     suffices k, M, G may be used in the usual way. The default is the node
     memory divided by the node processor count, except when -n specifies a
     smaller number of processes than in one node, and the destination is not
     htc, when the whole node memory will be used. To allow this mechanism to
     work properly, the Molpro input file should not normally contain any
     memory directives.

  -q The Slurm partition (default htc).

  -P SCW project number. If omitted, it will be looked for in the environment
     variable PROJECT. Note that it is also possible to place default options
     for ${this} in the file ${configfile}.

EOF
exit 1
fi



if [ ! -z $verbose ]; then
echo 'Processors:      ' $NOPTION
echo 'Tasks per node:  ' $TASKS_PER_NODE
echo 'Memory:          ' $MEMORY
fi;
DIR=`/bin/pwd`

mem=$(echo ${MEMORY}|sed -e 's/m/*1024*1024/' -e 's/g/*1024*1024*1024/' -e 's/$/\/1024\/1024))/' -e 's/^/import math; print int(math.ceil(8.0*/' |python2)
SBATCHARGS="$SBATCHARGS --ntasks=$NOPTION --mem-per-cpu=$mem --ntasks-per-node=${TASKS_PER_NODE} -p ${QUEUE} -A ${PROJECT}"
#SBATCHARGS="$SBATCHARGS --time=0-00:20 "

if [ ! -z $debug ]; then debug='export MOLPRO_GA_DEBUG=1' ; fi
export SCW_TPN_OVERRIDE=1

for file in $* ; do
jobname=`basename ${file}|sed -e 's/\.[a-zA-Z0-9]*$//'`
qjobname=`echo "$jobname" | sed -e 's,[-+.],,g' -e 's,\(..........\).*,\1,'`
echo sbatch $SBATCHARGS --job-name=${qjobname} --output=${jobname}-slurm-%J.out 
sbatch $SBATCHARGS --job-name=${qjobname} --output=${jobname}-slurm-%J.out << EOF
#! /bin/sh
echo "Project $PROJECT"
if [ -r ./.bashrc ]; then  . ./.bashrc ; fi
#module purge
# need to find a better way of ensuring the right modules
module load compiler/gnu/6
module load compiler/intel/2018/3
module load mpi/intel
for  i in \$(echo $LOADEDMODULES|sed -e 's/:/ /g') ; do module load \$i; done
module list
export MOLPRO_OPTIONS="$MOLPRO_OPTIONS"
export PATH=$PATH:\$JOBPATH:\$PATH
export SCRATCHDIR=${scratchdir}/SLURM.\${SLURM_JOBID}
# the right variable name?
export OMP_NUM_THREADS=\${THREADS:-1}
$debug
echo cd $PWD
cd $PWD
date
hostname
echo PATH=\$PATH
echo progpath=\$progpath
which $progpath
if [ $prog = molpro ]; then
echo $progpath -m ${MEMORY} -n $NOPTION -d \$SCRATCHDIR $OPTION $file
srun mkdir -p \$SCRATCHDIR
time $progpath -v -m ${MEMORY} -n $NOPTION -d \$SCRATCHDIR $OPTION $file
ls -l \$SCRATCHDIR
if test -z $keeptmp ; then
 srun rm -rf \$SCRATCHDIR
fi
else
echo $progpath $OPTION $file
time $progpath $OPTION $file
fi
env
df -h
vmstat -sSM
grep -c ^processor /proc/cpuinfo
srun hostname
EOF

done
