#!/bin/sh
prefix='q'
this=`basename $0`
prog=${this#${prefix}}

# configuration
#Include this if workers have different core count to head, or if you want for other reasons to configure manually
#max_tasks_per_node=16 

if [ -z "$max_tasks_per_node" ]; then uname=$(uname); test $uname = Darwin && max_tasks_per_node=$(sysctl -n hw.ncpu) || max_tasks_per_node=$(nproc||echo 1) ; fi

#default
defaultoptions="-P none -d ${TMPDIR:-/scratch/${USER}}"
                                                                                
# parse options
systemconfigfile=$0.rc
#echo systemconfigfile=$systemconfigfile
#ls -l $systemconfigfile
configfile=${HOME}/.${this}rc
test -r $systemconfigfile && systemconfigfileoptions=$(cat ${systemconfigfile})
test -r $configfile && configfileoptions=$(cat ${configfile})
#echo systemconfigfileoptions=$systemconfigfileoptions
#echo configfileoptions=$configfileoptions
#echo defaultoptions=$defaultoptions
set -- `getopt gd:vW:t:n:l:q:x:P:m:N: $defaultoptions $systemconfigfileoptions $configfileoptions $*`
parse_opts=0
MEMORY=
TASKS_PER_NODE=
while [ "$1" != -- ]; do
# echo option $1 $2
  case "$1" in
    -d) eval scratchdir="$2"; shift;;
    -g) debug=1 ;;
    -v) verbose=1 ;;
    -k) keeptmp=1 ;;
    -W) OPTION="$OPTION $2"; shift;;
    -l) QSUBARGS="$QSUBARGS -l $2"; shift;;
    -m) echo m option $2; MEMORY=$2; shift;;
    -n) NOPTION=$2; shift;;
    -N) TASKS_PER_NODE=$2; shift;;
    -t) OPTION="$OPTION $1 $2"; THREADS=$2; shift;;
    -q) QUEUE=$2; shift;;
    -P) PROJECT=$2; shift;;
    -x) garbage=$2; shift;;
     *) parse_opts=1;;
  esac
  shift
done
shift

# hardware
if [ x${QUEUE} = xcompute  -o x${QUEUE} = xdev ]; then
    hardware_memory=180
    hardware_cores=40
elif [ x${QUEUE} = xhighmem ]; then
    hardware_memory=372
    hardware_cores=40
else # should be HTC
    QUEUE=HTC
    hardware_memory=180
    hardware_cores=40
    if [ x${NOPTION} = x ]; then NOPTION=1 ; fi
    if [ x${MEMORY} = x ]; then
	MEMORY=$(echo "import math; print(int(math.floor(128.0*${hardware_memory}/${hardware_cores})))"|python)"m"
    fi
fi

if [ x${NOPTION} = x ]; then NOPTION=$hardware_cores ; fi
if [ x${TASKS_PER_NODE} = x ]; then
    if [ x${MEMORY} = x ]; then
	TASKS_PER_NODE=$hardware_cores
    else
	mem=$(echo ${MEMORY}|sed -e 's/m/*1024*1024/' -e 's/g/*1024*1024*1024/' -e 's/$/))/' -e 's/^/import math; print int(math.ceil(1.0*/' |python)
	TASKS_PER_NODE=$(echo "import math; print(int(math.floor(${hardware_memory}*1024.*1024.*1024./${mem})))"|python)
    fi
fi

if [ ${TASKS_PER_NODE} -gt ${NOPTION} ]; then TASKS_PER_NODE=${NOPTION}; fi

if [ x${MEMORY} = x ]; then
    MEMORY=$(echo "import math; print(int(math.floor(128.0*${hardware_memory}/${TASKS_PER_NODE})))"|python)"m"
fi

if [ x$verbose != x ]; then
    echo QUEUE=$QUEUE
    echo MEMORY=$MEMORY
    echo TASKS_PER_NODE=$TASKS_PER_NODE
    echo NOPTION=$NOPTION
fi
exit;
					       

if [ x${PROJECT} == xnone ]; then # use the special value 'none' in qmolpro.rc to denote that -P must be given later
  echo "Project missing. Specify it with -P (possible in $HOME/.${this}rc), or via the PROJECT environment variable"
  exit 1;
fi

if [ x${garbage} != x -a x${scratchdir} != x ]; then # garbage-collect any outstanding scratch directories
#fixme
    find $scratchdir -name 'PBS.*' -mtime -"$garbage" -print0 | xargs -0  rm -rf
fi

tasks_per_node=$max_tasks_per_node
cpus=`echo $NOPTION | sed -e 's/\/.*$//'`
tasks_per_node=`echo $NOPTION | sed -e 's/^.*\///' -e 's/:.*$//'`
if [ $tasks_per_node -eq $cpus ]; then
    task_flag="total"
    tasks_per_node=$max_tasks_per_node
elif [ $tasks_per_node -gt $max_tasks_per_node ]; then
    echo TASKS_PER_NODE too high, ignoring
    task_flag="total"
    tasks_per_node=$max_tasks_per_node
fi
typeset -i nodes
let nodes=($cpus-1)/$tasks_per_node+1;
if [ $nodes -eq 1 ]; then tasks_per_node=$cpus; fi
 
if [ $parse_opts -eq 1 ] ; then
cat << EOF
Usage: $SCRIPT -n CPUS/TASKS_PER_NODE -h HH[:MM[:SS]] [-v] [-k] -W MOLPRO_OPTION INPUT_FILES
EOF
exit 1
fi



if [ "x$cpus" != "x1" -o x${QUEUE:-serial} != xserial ]; then
  QSUBARGS="$QSUBARGS -l select=${nodes}:ncpus=${tasks_per_node}:mpiprocs=${tasks_per_node} "
  MEMORY=$(echo "import math; print(int(math.floor(8192.0/${tasks_per_node})))"|python)"m"
else
    if [ "x$MEMORY" != "x" ]; then
	mem=$(echo ${MEMORY}|sed -e 's/m/*1024*1024/' -e 's/g/*1024*1024*1024/' -e 's/$/\/128\/1024\/1024))/' -e 's/^/import math; print int(math.ceil(1.0*/' |python)
	QSUBARGS="$QSUBARGS -l select=1:ncpus=1:mem=${mem}g"
    fi
fi

if [ ! -z $verbose ]; then
echo 'Processors:      ' $cpus
echo 'Tasks per node:  ' $tasks_per_node
echo 'Nodes:           ' $nodes
echo 'Memory:          ' $MEMORY
fi;
DIR=`/bin/pwd`


if [ ! -z $debug ]; then debug='export MOLPRO_GA_DEBUG=1' ; fi
for file in $* ; do
jobname=`basename ${file}|sed -e 's/\.[a-zA-Z0-9]*$//'`
qjobname=`echo "$jobname" | sed -e 's,[-+.],,g' -e 's,\(..........\).*,\1,'`
echo qsub $QSUBARGS -j oe -N ${qjobname} -P $PROJECT
qsub $QSUBARGS -j oe -N ${qjobname} -P $PROJECT << EOF

echo "hello from start of job"
if [ -r .bashrc ]; then  . .bashrc ; fi
#module purge
# need to find a better way of ensuring the right modules
for  i in \$(echo $LOADEDMODULES|sed -e 's/:/ /g') ; do module load \$i; done
module list
export MOLPRO_OPTIONS="$MOLPRO_OPTIONS"
export PATH=$PATH:\$JOBPATH:\$PATH
export SCRATCHDIR=${scratchdir}/PBS.\${PBS_JOBID}
# the right variable name?
export OMP_NUM_THREADS=\${THREADS:-1}
$debug
echo cd $PWD
cd $PWD
date
hostname
echo PATH=\$PATH
which $prog
if [ $prog = molpro ]; then
echo $prog -v -d \$SCRATCHDIR $OPTION $file
pbsdsh -- mkdir -p \$SCRATCHDIR
time $prog -m ${MEMORY} -n $cpus -d \$SCRATCHDIR $OPTION $file
ls -l \$SCRATCHDIR
if test -z $keeptmp ; then
 pbsdsh -- rm -rf \$SCRATCHDIR
fi
else
echo $prog $OPTION $file
time $prog $OPTION $file
fi
env
df -h
EOF

done

#echo PBS_NODEFILE=\$PBS_NODEFILE
#cat \$PBS_NODEFILE
#echo ======================

#export PATH=$JOBPATH:/opt/molpro/bin:\$PATH:/usr/local/bin
